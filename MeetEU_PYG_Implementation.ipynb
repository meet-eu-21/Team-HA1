{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import sklearn\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,sys,pickle,time,random,glob\n",
    "sys.path.append(os.getcwd())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "import copy\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.utils.data\n",
    "#from torch_sparse import SparseTensor, cat\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data(x=torch.from_numpy(X_train_split_element.astype(\"float32\")), edge_index=torch.from_numpy(edge_index_train).type(torch.LongTensor), y = torch.from_numpy(obs_train_split_element).type(torch.LongTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(data_list_train, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"epoch_num\": 100,\n",
    "    \"batch_size\": 256,\n",
    "    \"number_attention_heads\": 8,\n",
    "    \"dropout\": 0.4,\n",
    "    \"number_hidden_units\": 8,\n",
    "    \"number_node_features\": 9,\n",
    "    \"number_classes\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, parameters):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATConv(parameters[\"number_node_features\"], out_channels=parameters[\"number_hidden_units\"],\n",
    "                            heads=parameters[\"number_attention_heads\"], concat=True, negative_slope=0.001,\n",
    "                            dropout=parameters[\"dropout\"], bias=True)\n",
    "        self.gat2 = GATConv(parameters[\"number_hidden_units\"] * parameters[\"number_attention_heads\"], parameters[\"number_classes\"], #n_nodes is missing\n",
    "                            heads=parameters[\"number_attention_heads\"], concat=False, negative_slope=0.001,\n",
    "                            dropout=parameters[\"dropout\"], bias=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #x = self.gat1(x.float(), edge_index.float())\n",
    "        #x = self.gat1(x.double(), edge_index.double())\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(\n",
    "    project=\"iGEM2021/GNN\",\n",
    "    tags=\"GNN01\",\n",
    "    api_token=\"\",\n",
    "    source_files=[\"*.py\"],\n",
    ")\n",
    "\n",
    "#neptune.create_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(parameters).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(),\n",
    "                                lr=parameters[\"learning_rate\"],\n",
    "                                weight_decay=parameters[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "run[\"parameters\"] = parameters\n",
    "model.train()   \n",
    "\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "epoch_roc = []\n",
    "outputs_train_list = []\n",
    "epoch_test_loss = []\n",
    "epoch_test_acc = []\n",
    "epoch_test_roc = []\n",
    "outputs_test_list = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "        \n",
    "    for graph_train_batch in dataloader_train:\n",
    "        graph_train_batch = graph_train_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(graph_train_batch) ########################\n",
    "        \n",
    "        max_val, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = criterion(outputs, graph_train_batch.y)\n",
    "        \n",
    "        acc = (torch.sum(preds == graph_train_batch.y)) / len(graph_train_batch.x)\n",
    "        \n",
    "        roc_score = roc_auc_score(graph_train_batch.y.clone().detach().cpu(), max_val.clone().detach().cpu()) #outputs.T[0].clone().detach().cpu()\n",
    "        \n",
    "        run[\"logs/training/batch/loss\"].log(loss)\n",
    "        print(\"Train loss\")\n",
    "        print(loss.item())\n",
    "        #neptune.log_metric('loss', loss)\n",
    "        run[\"logs/training/batch/acc\"].log(acc)\n",
    "        #neptune.log_metric('acc', acc)\n",
    "        print(\"Train accuracy\")\n",
    "        print(acc.item())\n",
    "        run[\"logs/training/batch/roc\"].log(roc_score)\n",
    "        #neptune.log_metric('roc', roc_score)\n",
    "        print(\"Train roc\")\n",
    "        print(roc_score)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_acc.append(acc.item())\n",
    "        epoch_roc.append(roc_score)\n",
    "        outputs_train_list.append(outputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #writer.add_scalar('Training loss step', scores, global_step = step)\n",
    "\n",
    "        step +=1\n",
    "        \n",
    "    #EVALUATION   \n",
    "    #if step % 8 == 0:\n",
    "    print(\"Evaluation\")\n",
    "    with torch.no_grad():           \n",
    "        model.eval()\n",
    "        scores = 0\n",
    "        dimension = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for graph_test_batch in dataloader_test:\n",
    "\n",
    "            graph_test_batch = graph_test_batch.to(device)\n",
    "\n",
    "            outputs = model(graph_test_batch)\n",
    "\n",
    "            max_val, preds = torch.max(outputs, 1)\n",
    "\n",
    "            loss = criterion(outputs, graph_test_batch.y)\n",
    "\n",
    "            acc = (torch.sum(preds == graph_test_batch.y)) / len(graph_test_batch.x)\n",
    "\n",
    "            roc_score = roc_auc_score(graph_test_batch.y.clone().detach().cpu(), max_val.clone().detach().cpu())\n",
    "\n",
    "            run[\"logs/test/batch/loss\"].log(loss)\n",
    "            #neptune.log_metric('test_loss', loss)\n",
    "            run[\"logs/test/batch/acc\"].log(acc)\n",
    "            #neptune.log_metric('test_acc', acc)\n",
    "            run[\"logs/test/batch/roc\"].log(roc_score)\n",
    "            #neptune.log_metric('roc', roc_score)\n",
    "\n",
    "            epoch_test_loss.append(loss.item())\n",
    "            epoch_test_acc.append(acc.item())\n",
    "            epoch_test_roc.append(roc_score)\n",
    "            \n",
    "            outputs_test_list.append(outputs)\n",
    "\n",
    "            #writer.add_scalar('Test loss step', scores/(dimension*output.size()[1]*output.size()[2]), global_step = step)\n",
    "\n",
    "        model.train()   \n",
    "\n",
    "        torch.save(model.state_dict(), f\"./{epoch}-{step}-{set}.pt\")\n",
    "        \n",
    "print(\"Max train loss epoch\")\n",
    "print(max(epoch_loss))\n",
    "print(\"Max train acc epoch\")\n",
    "print(max(epoch_acc))\n",
    "print(\"Max train roc epoch\")\n",
    "print(max(epoch_roc))\n",
    "print(\"Max test loss epoch\")\n",
    "print(max(epoch_test_loss))\n",
    "print(\"Max test acc epoch\")\n",
    "print(max(epoch_test_acc))\n",
    "print(\"Max test roc epoch\")\n",
    "print(max(epoch_test_roc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
